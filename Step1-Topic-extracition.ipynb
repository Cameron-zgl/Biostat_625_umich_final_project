{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eea198d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged: EDAnonymous_pre_features_tfidf_256.csv\n",
      "Merged: EDAnonymous_2019_features_tfidf_256.csv\n",
      "Merged: EDAnonymous_post_features_tfidf_256.csv\n",
      "Merged: addiction_pre_features_tfidf_256.csv\n",
      "Merged: addiction_2018_features_tfidf_256.csv\n",
      "Merged: addiction_post_features_tfidf_256.csv\n",
      "Merged: addiction_2019_features_tfidf_256.csv\n",
      "Merged: alcoholism_2018_features_tfidf_256.csv\n",
      "Merged: alcoholism_post_features_tfidf_256.csv\n",
      "Merged: alcoholism_2019_features_tfidf_256.csv\n",
      "Merged: alcoholism_pre_features_tfidf_256.csv\n",
      "Merged: adhd_post_features_tfidf_256.csv\n",
      "Merged: adhd_2019_features_tfidf_256.csv\n",
      "Merged: adhd_pre_features_tfidf_256.csv\n",
      "Merged: adhd_2018_features_tfidf_256.csv\n",
      "Merged: socialanxiety_pre_features_tfidf_256.csv\n",
      "Merged: anxiety_2019_features_tfidf_256.csv\n",
      "Merged: socialanxiety_2019_features_tfidf_256.csv\n",
      "Merged: socialanxiety_post_features_tfidf_256.csv\n",
      "Merged: anxiety_2018_features_tfidf_256.csv\n",
      "Merged: socialanxiety_2018_features_tfidf_256.csv\n",
      "Merged: anxiety_post_features_tfidf_256.csv\n",
      "Merged: healthanxiety_2018_features_tfidf_256.csv\n",
      "Merged: anxiety_pre_features_tfidf_256.csv\n",
      "Merged: healthanxiety_post_features_tfidf_256.csv\n",
      "Merged: healthanxiety_2019_features_tfidf_256.csv\n",
      "Merged: healthanxiety_pre_features_tfidf_256.csv\n",
      "Merged: autism_2019_features_tfidf_256.csv\n",
      "Merged: autism_post_features_tfidf_256.csv\n",
      "Merged: autism_2018_features_tfidf_256.csv\n",
      "Merged: autism_pre_features_tfidf_256.csv\n",
      "Merged: bipolarreddit_2019_features_tfidf_256.csv\n",
      "Merged: bipolarreddit_2018_features_tfidf_256.csv\n",
      "Merged: bipolarreddit_pre_features_tfidf_256.csv\n",
      "Merged: bipolarreddit_post_features_tfidf_256.csv\n",
      "Merged: bpd_post_features_tfidf_256.csv\n",
      "Merged: bpd_2019_features_tfidf_256.csv\n",
      "Merged: bpd_2018_features_tfidf_256.csv\n",
      "Merged: bpd_pre_features_tfidf_256.csv\n",
      "Merged: depression_pre_features_tfidf_256.csv\n",
      "Merged: depression_2019_features_tfidf_256.csv\n",
      "Merged: depression_2018_features_tfidf_256.csv\n",
      "Merged: depression_post_features_tfidf_256.csv\n",
      "Merged: healthanxiety_2018_features_tfidf_256.csv\n",
      "Merged: healthanxiety_post_features_tfidf_256.csv\n",
      "Merged: healthanxiety_2019_features_tfidf_256.csv\n",
      "Merged: healthanxiety_pre_features_tfidf_256.csv\n",
      "Merged: lonely_post_features_tfidf_256.csv\n",
      "Merged: lonely_2018_features_tfidf_256.csv\n",
      "Merged: lonely_2019_features_tfidf_256.csv\n",
      "Merged: lonely_pre_features_tfidf_256.csv\n",
      "Merged: ptsd_pre_features_tfidf_256.csv\n",
      "Merged: ptsd_2018_features_tfidf_256.csv\n",
      "Merged: ptsd_post_features_tfidf_256.csv\n",
      "Merged: ptsd_2019_features_tfidf_256.csv\n",
      "Merged: schizophrenia_post_features_tfidf_256.csv\n",
      "Merged: schizophrenia_2019_features_tfidf_256.csv\n",
      "Merged: schizophrenia_2018_features_tfidf_256.csv\n",
      "Merged: schizophrenia_pre_features_tfidf_256.csv\n",
      "Merged: socialanxiety_pre_features_tfidf_256.csv\n",
      "Merged: socialanxiety_2019_features_tfidf_256.csv\n",
      "Merged: socialanxiety_post_features_tfidf_256.csv\n",
      "Merged: socialanxiety_2018_features_tfidf_256.csv\n",
      "Merged: suicidewatch_pre_features_tfidf_256.csv\n",
      "Merged: suicidewatch_2019_features_tfidf_256.csv\n",
      "Merged: suicidewatch_post_features_tfidf_256.csv\n",
      "Merged: suicidewatch_2018_features_tfidf_256.csv\n",
      "Saved: mental_health_support.csv\n",
      "Total files merged for mental_health_support: 67\n",
      "Merged: mentalhealth_pre_features_tfidf_256.csv\n",
      "Merged: mentalhealth_post_features_tfidf_256.csv\n",
      "Merged: mentalhealth_2018_features_tfidf_256.csv\n",
      "Merged: mentalhealth_2019_features_tfidf_256.csv\n",
      "Merged: COVID19_support_post_features_tfidf_256.csv\n",
      "Saved: broad_mental_health.csv\n",
      "Total files merged for broad_mental_health: 5\n",
      "Merged: conspiracy_post_features_tfidf_256.csv\n",
      "Merged: conspiracy_2019_features_tfidf_256.csv\n",
      "Merged: conspiracy_pre_features_tfidf_256.csv\n",
      "Merged: conspiracy_2018_features_tfidf_256.csv\n",
      "Merged: divorce_2019_features_tfidf_256.csv\n",
      "Merged: divorce_2018_features_tfidf_256.csv\n",
      "Merged: divorce_post_features_tfidf_256.csv\n",
      "Merged: divorce_pre_features_tfidf_256.csv\n",
      "Merged: fitness_2019_features_tfidf_256.csv\n",
      "Merged: fitness_post_features_tfidf_256.csv\n",
      "Merged: fitness_2018_features_tfidf_256.csv\n",
      "Merged: fitness_pre_features_tfidf_256.csv\n",
      "Merged: guns_2019_features_tfidf_256.csv\n",
      "Merged: guns_post_features_tfidf_256.csv\n",
      "Merged: guns_2018_features_tfidf_256.csv\n",
      "Merged: guns_pre_features_tfidf_256.csv\n",
      "Merged: jokes_pre_features_tfidf_256.csv\n",
      "Merged: jokes_2019_features_tfidf_256.csv\n",
      "Merged: jokes_post_features_tfidf_256.csv\n",
      "Merged: jokes_2018_features_tfidf_256.csv\n",
      "Merged: legaladvice_pre_features_tfidf_256.csv\n",
      "Merged: legaladvice_2018_features_tfidf_256.csv\n",
      "Merged: legaladvice_post_features_tfidf_256.csv\n",
      "Merged: legaladvice_2019_features_tfidf_256.csv\n",
      "Merged: meditation_2019_features_tfidf_256.csv\n",
      "Merged: meditation_2018_features_tfidf_256.csv\n",
      "Merged: meditation_post_features_tfidf_256.csv\n",
      "Merged: meditation_pre_features_tfidf_256.csv\n",
      "Merged: parenting_2018_features_tfidf_256.csv\n",
      "Merged: parenting_post_features_tfidf_256.csv\n",
      "Merged: parenting_2019_features_tfidf_256.csv\n",
      "Merged: parenting_pre_features_tfidf_256.csv\n",
      "Merged: personalfinance_post_features_tfidf_256.csv\n",
      "Merged: personalfinance_pre_features_tfidf_256.csv\n",
      "Merged: personalfinance_2018_features_tfidf_256.csv\n",
      "Merged: personalfinance_2019_features_tfidf_256.csv\n",
      "Merged: relationships_2019_features_tfidf_256.csv\n",
      "Merged: relationships_pre_features_tfidf_256.csv\n",
      "Merged: relationships_2018_features_tfidf_256.csv\n",
      "Merged: relationships_post_features_tfidf_256.csv\n",
      "Merged: teaching_2019_features_tfidf_256.csv\n",
      "Merged: teaching_post_features_tfidf_256.csv\n",
      "Merged: teaching_pre_features_tfidf_256.csv\n",
      "Merged: teaching_2018_features_tfidf_256.csv\n",
      "Saved: non_mental_health.csv\n",
      "Total files merged for non_mental_health: 44\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder containing the data files\n",
    "data_folder = \"data\"\n",
    "\n",
    "# Define groups of subreddits\n",
    "group_mental_health_support = [\n",
    "    \"EDAnonymous\", \"addiction\", \"alcoholism\", \"adhd\", \"anxiety\",\n",
    "    \"autism\", \"bipolarreddit\", \"bpd\", \"depression\", \"healthanxiety\",\n",
    "    \"lonely\", \"ptsd\", \"schizophrenia\", \"socialanxiety\", \"suicidewatch\"\n",
    "]\n",
    "\n",
    "group_broad_mental_health = [\"mentalhealth\", \"COVID19_support\"]\n",
    "\n",
    "group_non_mental_health = [\n",
    "    \"conspiracy\", \"divorce\", \"fitness\", \"guns\", \"jokes\",\n",
    "    \"legaladvice\", \"meditation\", \"parenting\", \"personalfinance\",\n",
    "    \"relationships\", \"teaching\"\n",
    "]\n",
    "\n",
    "# Define a function to merge CSV files\n",
    "def merge_csv_files(group, output_name):\n",
    "    combined_data = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "    file_count = 0  # Counter for the number of files merged\n",
    "\n",
    "    for subreddit in group:\n",
    "        # Find all matching files for the given subreddit\n",
    "        files = [f for f in os.listdir(data_folder) if subreddit in f and f.endswith(\".csv\")]\n",
    "        for file in files:\n",
    "            file_path = os.path.join(data_folder, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "            print(f\"Merged: {file}\")  # Print the name of the file being merged\n",
    "            file_count += 1  # Increment the file counter\n",
    "\n",
    "    # Save the combined DataFrame to a new CSV file\n",
    "    combined_data.to_csv(f\"{output_name}.csv\", index=False)\n",
    "    print(f\"Saved: {output_name}.csv\")\n",
    "    print(f\"Total files merged for {output_name}: {file_count}\")  # Print the total number of files merged\n",
    "\n",
    "# Merge and save CSV files for each group\n",
    "merge_csv_files(group_mental_health_support, \"mental_health_support\")\n",
    "merge_csv_files(group_broad_mental_health, \"broad_mental_health\")\n",
    "merge_csv_files(group_non_mental_health, \"non_mental_health\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c77d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: mental_health_support.csv\n",
      "Processed and saved: broad_mental_health.csv\n",
      "Processed and saved: non_mental_health.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of processed CSV files\n",
    "output_files = [\"mental_health_support.csv\", \"broad_mental_health.csv\", \"non_mental_health.csv\"]\n",
    "\n",
    "# Process each file\n",
    "for file in output_files:\n",
    "    # Read the dataset\n",
    "    data = pd.read_csv(file)\n",
    "    \n",
    "    # Keep only the first four columns\n",
    "    data = data.iloc[:, :4]\n",
    "    \n",
    "    # Save the processed dataset, overwriting the original file\n",
    "    data.to_csv(file, index=False)\n",
    "    print(f\"Processed and saved: {file}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d90b9546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     active environment : None\n",
      "       user config file : /home/gulizhu/.condarc\n",
      " populated config files : \n",
      "          conda version : 4.10.3\n",
      "    conda-build version : 3.21.5\n",
      "         python version : 3.9.7.final.0\n",
      "       virtual packages : __cuda=12.4=0\n",
      "                          __linux=4.18.0=0\n",
      "                          __glibc=2.28=0\n",
      "                          __unix=0=0\n",
      "                          __archspec=1=x86_64\n",
      "       base environment : /sw/pkgs/arc/python3.9-anaconda/2021.11  (read only)\n",
      "      conda av data dir : /sw/pkgs/arc/python3.9-anaconda/2021.11/etc/conda\n",
      "  conda av metadata url : None\n",
      "           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "          package cache : /sw/pkgs/arc/python3.9-anaconda/2021.11/pkgs\n",
      "                          /home/gulizhu/.conda/pkgs\n",
      "       envs directories : /home/gulizhu/.conda/envs\n",
      "                          /sw/pkgs/arc/python3.9-anaconda/2021.11/envs\n",
      "               platform : linux-64\n",
      "             user-agent : conda/4.10.3 requests/2.26.0 CPython/3.9.7 Linux/4.18.0-477.51.1.el8_8.x86_64 rhel/8.8 glibc/2.28\n",
      "                UID:GID : 114407184:114407184\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n",
      "# conda environments:\n",
      "#\n",
      "base                  *  /sw/pkgs/arc/python3.9-anaconda/2021.11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda info\n",
    "!conda env list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92cee2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec  2 14:43:02 2024       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  Tesla V100-PCIE-16GB           On  |   00000000:D8:00.0 Off |                    0 |\r\n",
      "| N/A   34C    P0             38W /  250W |       0MiB /  16384MiB |      0%   E. Process |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f28b6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the dataset:\n",
      "         subreddit     author        date  \\\n",
      "count       458144     458144      458144   \n",
      "unique          15     314304         587   \n",
      "top     depression  [deleted]  2020/01/01   \n",
      "freq        117331         29        2056   \n",
      "\n",
      "                                                     post  \n",
      "count                                              458144  \n",
      "unique                                             394431  \n",
      "top     has anyone taken olanzapine/zyprexa or valproa...  \n",
      "freq                                                   12  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"mental_health_support.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the summary of the data\n",
    "print(\"Summary of the dataset:\")\n",
    "print(data.describe(include='all'))  # Includes numeric and non-numeric columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52ff6720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (1.3.4)\n",
      "Requirement already satisfied: nltk in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (3.6.5)\n",
      "Requirement already satisfied: scikit-learn in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (0.24.2)\n",
      "Requirement already satisfied: matplotlib in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (3.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: click in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from scikit-learn) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: six in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas nltk scikit-learn matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e1431ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gulizhu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gulizhu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gulizhu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/tmp/ipykernel_2774315/1829259800.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['date'] = pd.to_datetime(filtered_data['date'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing half-year: 2018-1\n",
      "Keywords for 2018-1:\n",
      "  Cluster 0:\n",
      "    Top 10 Unigrams: ['depression', 'get', 'help', 'day', 'like', 'know', 'feel', 'anyone', 'time', 'year']\n",
      "    Top 10 Bigrams: ['anyone else', 'feel like', 'side effect', 'depression anxiety', 'mental health', 'first time', 'need help', 'suicidal thought', 'gon na', 'wan na']\n",
      "  Cluster 1:\n",
      "    Top 10 Unigrams: ['feel', 'like', 'know', 'feeling', 'people', 'want', 'life', 'really', 'even', 'time']\n",
      "    Top 10 Bigrams: ['feel like', 'make feel', 'anyone else', 'else feel', 'feeling like', 'feel way', 'feel better', 'feel alone', 'felt like', 'like shit']\n",
      "  Cluster 2:\n",
      "    Top 10 Unigrams: ['want', 'fucking', 'hate', 'life', 'die', 'ca', 'know', 'people', 'anymore', 'feel']\n",
      "    Top 10 Bigrams: ['want die', 'feel like', 'wan na', 'want kill', 'want live', 'fucking hate', 'gon na', 'really want', 'want go', 'every day']\n",
      "  Cluster 3:\n",
      "    Top 10 Unigrams: ['im', 'dont', 'cant', 'feel', 'like', 'ive', 'know', 'want', 'friend', 'life']\n",
      "    Top 10 Bigrams: ['dont know', 'feel like', 'dont want', 'like im', 'know im', 'think im', 'wan na', 'im going', 'dont even', 'im tired']\n",
      "  Cluster 4:\n",
      "    Top 10 Unigrams: ['friend', 'year', 'like', 'know', 'time', 'get', 'life', 'feel', 'really', 'even']\n",
      "    Top 10 Bigrams: ['feel like', 'high school', 'year ago', 'last year', 'year old', 'best friend', 'even though', 'every day', 'first time', 'felt like']\n",
      "Processing half-year: 2018-2\n",
      "Keywords for 2018-2:\n",
      "  Cluster 0:\n",
      "    Top 10 Unigrams: ['fucking', 'hate', 'want', 'fuck', 'die', 'life', 'shit', 'people', 'know', 'like']\n",
      "    Top 10 Bigrams: ['want die', 'wan na', 'feel like', 'piece shit', 'hate life', 'gon na', 'wish could', 'hate hate', 'fucking hate', 'fuck fuck']\n",
      "  Cluster 1:\n",
      "    Top 10 Unigrams: ['depression', 'anyone', 'feel', 'like', 'help', 'know', 'life', 'day', 'else', 'get']\n",
      "    Top 10 Bigrams: ['anyone else', 'feel like', 'amp x200b', 'http http', 'mental health', 'wish could', 'side effect', 'depression anxiety', 'need help', 'feel better']\n",
      "  Cluster 2:\n",
      "    Top 10 Unigrams: ['feel', 'like', 'know', 'want', 'people', 'life', 'even', 'get', 'really', 'feeling']\n",
      "    Top 10 Bigrams: ['feel like', 'make feel', 'anyone else', 'still feel', 'feel better', 'get better', 'feeling like', 'feel way', 'wan na', 'feel empty']\n",
      "  Cluster 3:\n",
      "    Top 10 Unigrams: ['im', 'dont', 'ive', 'feel', 'want', 'like', 'cant', 'know', 'friend', 'even']\n",
      "    Top 10 Bigrams: ['dont know', 'dont want', 'feel like', 'like im', 'dont even', 'im going', 'im scared', 'im still', 'dont think', 'know im']\n",
      "  Cluster 4:\n",
      "    Top 10 Unigrams: ['year', 'get', 'friend', 'time', 'know', 'life', 'like', 'really', 'day', 'school']\n",
      "    Top 10 Bigrams: ['amp x200b', 'feel like', 'year old', 'year ago', 'high school', 'first time', 'best friend', 'every day', 'last year', 'month ago']\n",
      "Processing half-year: 2019-1\n",
      "Keywords for 2019-1:\n",
      "  Cluster 0:\n",
      "    Top 10 Unigrams: ['amp', 'x200b', 'feel', 'like', 'life', 'depression', 'time', 'know', 'want', 'year']\n",
      "    Top 10 Bigrams: ['amp x200b', 'x200b amp', 'feel like', 'http amp', 'x200b feel', 'x200b know', 'x200b anyone', 'life amp', 'anyone else', 'make feel']\n",
      "  Cluster 1:\n",
      "    Top 10 Unigrams: ['depression', 'want', 'get', 'know', 'life', 'day', 'like', 'help', 'feel', 'anyone']\n",
      "    Top 10 Bigrams: ['anyone else', 'feel like', 'wan na', 'want die', 'gon na', 'mental health', 'first time', 'get better', 'depression anxiety', 'suicidal thought']\n",
      "  Cluster 2:\n",
      "    Top 10 Unigrams: ['life', 'friend', 'year', 'know', 'like', 'get', 'time', 'want', 'feel', 'really']\n",
      "    Top 10 Bigrams: ['feel like', 'high school', 'year ago', 'year old', 'get better', 'best friend', 'every day', 'last year', 'even though', 'first time']\n",
      "  Cluster 3:\n",
      "    Top 10 Unigrams: ['feel', 'like', 'want', 'know', 'people', 'feeling', 'life', 'even', 'really', 'time']\n",
      "    Top 10 Bigrams: ['feel like', 'make feel', 'anyone else', 'else feel', 'feeling like', 'feel way', 'want feel', 'feel better', 'know feel', 'feel alone']\n",
      "  Cluster 4:\n",
      "    Top 10 Unigrams: ['im', 'dont', 'cant', 'feel', 'like', 'ive', 'want', 'know', 'life', 'friend']\n",
      "    Top 10 Bigrams: ['dont know', 'dont want', 'feel like', 'like im', 'im going', 'im tired', 'wan na', 'know im', 'dont even', 'think im']\n",
      "Processing half-year: 2019-2\n",
      "Keywords for 2019-2:\n",
      "  Cluster 0:\n",
      "    Top 10 Unigrams: ['depression', 'help', 'day', 'get', 'like', 'feel', 'know', 'anyone', 'time', 'need']\n",
      "    Top 10 Bigrams: ['anyone else', 'feel like', 'side effect', 'mental health', 'first time', 'need help', 'depression anxiety', 'suicidal thought', 'every day', 'wan na']\n",
      "  Cluster 1:\n",
      "    Top 10 Unigrams: ['im', 'dont', 'cant', 'ive', 'feel', 'like', 'want', 'know', 'friend', 'life']\n",
      "    Top 10 Bigrams: ['dont know', 'feel like', 'dont want', 'like im', 'im tired', 'im going', 'wan na', 'know im', 'im scared', 'think im']\n",
      "  Cluster 2:\n",
      "    Top 10 Unigrams: ['want', 'life', 'fucking', 'hate', 'ca', 'die', 'anymore', 'tired', 'get', 'feel']\n",
      "    Top 10 Bigrams: ['want die', 'wan na', 'feel like', 'want kill', 'gon na', 'wish could', 'want go', 'hate life', 'want live', 'every day']\n",
      "  Cluster 3:\n",
      "    Top 10 Unigrams: ['feel', 'like', 'know', 'feeling', 'want', 'get', 'life', 'time', 'people', 'even']\n",
      "    Top 10 Bigrams: ['feel like', 'make feel', 'anyone else', 'else feel', 'feeling like', 'know feel', 'like shit', 'feel better', 'time feel', 'life feel']\n",
      "  Cluster 4:\n",
      "    Top 10 Unigrams: ['friend', 'like', 'year', 'know', 'time', 'life', 'feel', 'really', 'get', 'even']\n",
      "    Top 10 Bigrams: ['feel like', 'amp x200b', 'high school', 'year ago', 'best friend', 'year old', 'get better', 'even though', 'last year', 'every day']\n",
      "Processing half-year: 2020-1\n",
      "Keywords for 2020-1:\n",
      "  Cluster 0:\n",
      "    Top 10 Unigrams: ['depression', 'help', 'feel', 'get', 'like', 'know', 'day', 'life', 'time', 'anyone']\n",
      "    Top 10 Bigrams: ['feel like', 'anyone else', 'mental health', 'wan na', 'gon na', 'side effect', 'amp x200b', 'need help', 'depression anxiety', 'get better']\n",
      "  Cluster 1:\n",
      "    Top 10 Unigrams: ['im', 'dont', 'cant', 'feel', 'like', 'ive', 'want', 'know', 'life', 'really']\n",
      "    Top 10 Bigrams: ['dont know', 'dont want', 'feel like', 'like im', 'im tired', 'im going', 'wan na', 'know im', 'think im', 'gon na']\n",
      "  Cluster 2:\n",
      "    Top 10 Unigrams: ['want', 'fucking', 'hate', 'life', 'die', 'know', 'tired', 'ca', 'anymore', 'people']\n",
      "    Top 10 Bigrams: ['want die', 'wan na', 'feel like', 'want live', 'want kill', 'gon na', 'fucking hate', 'really want', 'hate life', 'get better']\n",
      "  Cluster 3:\n",
      "    Top 10 Unigrams: ['year', 'friend', 'like', 'time', 'know', 'get', 'life', 'feel', 'really', 'even']\n",
      "    Top 10 Bigrams: ['feel like', 'high school', 'year ago', 'year old', 'last year', 'every day', 'best friend', 'mental health', 'get better', 'even though']\n",
      "  Cluster 4:\n",
      "    Top 10 Unigrams: ['feel', 'like', 'know', 'want', 'feeling', 'people', 'even', 'really', 'life', 'make']\n",
      "    Top 10 Bigrams: ['feel like', 'make feel', 'feeling like', 'feel way', 'anyone else', 'feel alone', 'feel empty', 'feel better', 'know feel', 'want feel']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"mental_health_support.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Filter data for a specific subgroup\n",
    "subreddit = \"depression\"\n",
    "filtered_data = data[data['subreddit'] == subreddit]\n",
    "\n",
    "# Convert 'date' to datetime and sort\n",
    "filtered_data['date'] = pd.to_datetime(filtered_data['date'], errors='coerce')\n",
    "filtered_data = filtered_data.dropna(subset=['date']).sort_values('date')\n",
    "\n",
    "# Group data by half-year\n",
    "filtered_data['half_year'] = filtered_data['date'].apply(lambda x: f\"{x.year}-{(x.month-1)//6 + 1}\")\n",
    "grouped_data = filtered_data.groupby('half_year')\n",
    "\n",
    "# Preprocessing function for text\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization\n",
    "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Function to extract keywords from clusters\n",
    "def extract_keywords(tfidf_matrix, feature_names, cluster_labels, top_n=10):\n",
    "    clusters = {}\n",
    "    for cluster in set(cluster_labels):\n",
    "        cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster]\n",
    "        cluster_tfidf = tfidf_matrix[cluster_indices].mean(axis=0).tolist()[0]\n",
    "        top_features = sorted(list(zip(feature_names, cluster_tfidf)), key=lambda x: x[1], reverse=True)\n",
    "        clusters[cluster] = [word for word, score in top_features[:top_n]]\n",
    "    return clusters\n",
    "\n",
    "# Process each half-year\n",
    "for half_year, posts in grouped_data:\n",
    "    print(f\"Processing half-year: {half_year}\")\n",
    "    \n",
    "    # Preprocess the text\n",
    "    posts_text = posts['post'].dropna().apply(preprocess_text).tolist()\n",
    "    \n",
    "    # TF-IDF feature extraction for unigrams\n",
    "    tfidf_vectorizer_unigrams = TfidfVectorizer(max_features=5000, ngram_range=(1, 1))\n",
    "    tfidf_matrix_unigrams = tfidf_vectorizer_unigrams.fit_transform(posts_text)\n",
    "    feature_names_unigrams = tfidf_vectorizer_unigrams.get_feature_names()  # Compatible with older versions\n",
    "\n",
    "    # TF-IDF feature extraction for bigrams\n",
    "    tfidf_vectorizer_bigrams = TfidfVectorizer(max_features=5000, ngram_range=(2, 2))\n",
    "    tfidf_matrix_bigrams = tfidf_vectorizer_bigrams.fit_transform(posts_text)\n",
    "    feature_names_bigrams = tfidf_vectorizer_bigrams.get_feature_names()  # Compatible with older versions\n",
    "\n",
    "    \n",
    "    # K-Means clustering\n",
    "    num_clusters = 5  # Number of clusters\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(tfidf_matrix_unigrams)\n",
    "    \n",
    "    # Extract keywords for unigrams\n",
    "    keywords_unigrams = extract_keywords(tfidf_matrix_unigrams, feature_names_unigrams, cluster_labels, top_n=10)\n",
    "    \n",
    "    # Extract keywords for bigrams\n",
    "    keywords_bigrams = extract_keywords(tfidf_matrix_bigrams, feature_names_bigrams, cluster_labels, top_n=10)\n",
    "    \n",
    "    # Print results for this half-year\n",
    "    print(f\"Keywords for {half_year}:\")\n",
    "    for cluster in keywords_unigrams.keys():\n",
    "        print(f\"  Cluster {cluster}:\")\n",
    "        print(f\"    Top 10 Unigrams: {keywords_unigrams[cluster]}\")\n",
    "        print(f\"    Top 10 Bigrams: {keywords_bigrams[cluster]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b572f37e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
